# Introdu√ß√£o a √âtica de Dados

|![ Sketchnote por [(@sketchthedocs)](https://sketchthedocs.dev) ](../../../sketchnotes/02-Ethics.png)|
|:---:|
| √âtica em Ci√™ncia de Dados - _Sketchnote por [@nitya](https://twitter.com/nitya)_ |

---

N√≥s somos todos cidad√£os dos dados vivendo em um mundo de dados.

Tend√™ncias do mercado nos mostram que at√© 2022, 1 em 3 grandes organiza√ß√µes ir√° comprar e vender seus dados atrav√©s de [Marketplaces e Exchanges](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) online. Como **Desenvolvedores de Aplicativos**, n√≥s vamos achar mais f√°cil e mais barato integrar insights baseados em dados e automa√ß√µes baseadas em algoritmos nas experi√™ncias di√°rias dos usu√°rio. Mas conforme IA se torna mais difundida, n√≥s tamb√©m vamos precisar entender os danos potenciais causado pelo uso desses algoritmos [como uma arma](https://www.youtube.com/watch?v=TQHs8SA1qpk).

Tend√™ncias tamb√©m indicam que n√≥s vamos criar e consumir mais de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de dados em 2025. Como **Cientistas de Dados**, isso nos dar√° n√≠veis de acesso sem precedentes √† dados pessoais. Isso significa que poderemos construir perfis comportamentais dos usu√°rio e influenciar tomadas de decis√£o de uma forma que crie a [ilus√£o da livre escolha](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) enquanto potencialmente direcionando os usu√°rios na dire√ß√£o do resultado que n√≥s preferimos. Isso tamb√©m levanta quest√µes mais amplas sobre privacidade dos dados e prote√ß√£o dos usu√°rios.

√âtica dos dados √© agora uma _prote√ß√£o necess√°rio_ para ci·∫Ωncia de dados e engenharia, nos ajudando a minimizar potenciais danos e consequ√™ncias n√£o intencionas das nossas a√ß√µes realizadas com base em dados. O [Gartner Hype Cycle for AI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifica tend√™ncias relevantes n√° √©tica digital, IAs respons√°veis, e governan√ßas de IA como principais impulsionadores para grandes mega tend√™ncias sobre _democratiza√ß√£o_ e _industrializa√ß√£o_ da IA.

![Gartner's Hype Cycle for AI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

Nessa aula, n√≥s vamos explorar a √°rea fascinante de √©tica dos dados - desde conceitos essenciais e desafios, para estudos de caso e conceitos de IA aplicados como governan√ßa - isso ajuda a estabelecer a cultura da √©tica nos times e organiza√ß√µes que trabalham com dados e IA.




## [Quiz pr√© aula](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/2) üéØ

## Defini√ß√£o B√°sica

Vamos come√ßar entendendo o b√°sico da terminologia.

A palavra "√©tica" vem da [palavra Grega "ethikos"](https://en.wikipedia.org/wiki/Ethics) (e sua ra√≠z "ethos") que significa _car√°ter ou natureza moral_.

**√âtica** √© sobre os valores e princ√≠pios morais compartilhados que governam o nosso comportamento em sociedade. √âtica √© baseada n√£o nas leis mas nas normas amplamente aceitas sobre o que √© "certo vs. errado". No entanto, considera√ß√µes √©ticas podem influenciar iniciativas de governan√ßa corporativa e regulamenta√ß√µes governamentais que criam mais incentivos para conformidade (compliance).

**√âtica de Dados** √© uma [nova ramifica√ß√£o da √©tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estuda e avalia problemas morais relacionados a _dados, algoritmos e pr√°ticas correspondentes_". Aqui, **"dados"** focam nas a√ß√µes relacionadas a gera√ß√£o, grava√ß√£o, curadoria, dissemina√ß√£o de processamento, compartilhamento, e uso, **"algoritmos"** focam em IA, agentes, aprendizado de m√°quina, e rob√¥s, e **"pr√°ticas"** focam em t√≥picos como inova√ß√£o respons√°vel, programa√ß√£o, hacking e c√≥digos de √©tica.

**√âtica Aplicada** √© a [aplica√ß√£o pr√°tica de considera√ß√µes morais](https://en.wikipedia.org/wiki/Applied_ethics). √â o processo de investigar ativamente problem√°ticas √©ticas no contexto de _a√ß√µes do mundo real, produtos e processos_, e tomar medidas corretivas para fazer com que esses permanecam alianhados com o nossos valores √©ticos definidos.

**Cultura √âtica** √© sobre [operacionalizar a √©tica aplicada](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para garantir que nossos princ√≠pios e pr√°ticas √©ticas sejam adotados de maneira consistente e escal√°vel em toda a organiza√ß√£o. Culturas √©ticas de sucesso definem princ√≠pios √©ticos em toda a organiza√ß√£o, fornecem incentivos significativos para consist√™ncia, e reinfor√ßa as normas √©ticas encorajando e amplificando comportmentos desejados em todos os n√≠veis da organiza√ß√£o.


## Conceitos √âticos

Nessa se√ß√£o, n√≥s vamos discutir conceitos como **valores compartilhados** (princ√≠pios) e **desafios √©ticos** (problemas) para a √©tica de dados - e explorar **estudos de caso** que ajudam voc√™ a entender esses conceitos em contextos do mundo real.

### 1. Princ√≠pios √âticos

Toda estrat√©gia de √©tica de dados come√ßa definindo _pric√≠pios √©ticos_ - os "valores compartilhados" que descrevem comportamentos aceit√°veis, e guia a√ß√µes complacentes, nos nossos dados e nos projetos de IA. Voc√™ pode definir eles individualmente ou com um time. No entando, a maioria das grandes organiza√ß√µes descreve eles em uma declara√ß√£o de miss√£o ou de estrutura de _IA √©tica_ que √© definida em n√≠veis corporativos e aplicadas consistentemente em todos os times.

**Exemplo:** a declara√ß√£o de miss√£o da [IA respons√°vel](https://www.microsoft.com/pt-br/ai/responsible-ai?activetab=pivot1:primaryr6) da Microsoft afirma: _"Estamos comprometidos com o avan√ßo da AI impulsionados por princ√≠pios √©ticos que colocam as pessoas em primeiro lugar."_ - identificando 6 princ√≠pios √©ticos na estrutura abaixo:

![IA Respon≈õavel na Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Vamos explorar brevemente esses princ√≠pios. _Transpar√™ncia_ e _responsabilidade_ s√£o valores fundamentais nos quais outros princ√≠pios constru√≠ram sobre - ent√£o vamos come√ßar a√≠:

* [**Responsabilidade**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) torna os profissionais _respons√°veis_ pelos seus dados e opera√ß√µes da IA, e conformidade (compliance) com esses princ√≠pios √©ticos.
* [**Transpar√™ncia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) garante que os dados e as a√ß√µes da IA s√£o _comprees√≠veis_ (interpret√°veis) para os usu√°rios, explicando o que e o porqu√™ por tr√°s de cada decis√£o.
* [**Justi√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - foca em garantir que a IA _trate_ todas as pessoas de forma justa, abordando quaisquer preconceitos sociot√©cnicos impl√≠citos ou sist√™micos nos dados e sistemas.
* [**Confiabilidade e Seguran√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - garante que a IA comporte de maneira _consistente_ com os valores definidos, minimizando potenciais danos ou consequ√™ncias n√£o pretendidas.
* [**Seguran√ßa e Privacidade**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - √© sobre compreender as linhagem dos dados, e fornecer _privacidade de dados e prote√ß√µes relacionadas_ aos usu√°rios.
* [**Inclus√£o**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - √© sobre projetar solu√ß√µes de IA com inten√ß√£o, adaptando elas para atender uma _vasta game de necessidades humanas_ & capacidades.

> üö® Pense sobre qual poderia ser a frase de miss√£o da sua √©tica de dados. Explore estruturas √©ticas de IA de outras organiza√ß√µes - aqui est√£o alguns exemplos da [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles), e [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Quais valores compartilhados voc√™s tem em comum? Como esses princ√≠pios se relacionam ao produto de IA ou √† ind√∫stria na qual eles operam?

### 2. Desafios de √âtica

Uma vez que nossos princ√≠pios √©ticos est√£o definidos, o pr√≥ximo passo √© avaliar nossos dados e a√ß√µes da IA  para ver se eles est√£o alinhados com aqueles valores compartilhados. Pense sobre suas a√ß√µes em duas categorias: _cole√ß√£o de dados_ e _design de algoritmo_.

Com cole√ß√µes dados, a√ß√µes ir√£o, provavelmente, envolver **dados pessoais** ou informa√ß√£o pessoalmente identific√°vel (do Ingl√™s, personally identifiable information, ou PII) para indiv√≠duos vivos identific√°veis. Isso inclui [itens diversos de dados n√£o pessoais](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que _coletivamente_ identificam um indiv√≠duo. Desafios √©ticos podem estar relacionados √† _privacidade dos dados_, _qualidade dos dados_, e t√≥picos relacionados como _consentimento informado_ e _direitos de propriedades intelectuais_ para os usu√°rios.

Com o design de algoritmo, as a√ß√µes envolver√£o coleta e curadoria dos **datasets**, e ent√£o o uso deles para treinar e implantar **modelos de dados** que predizem resultados ou automatizam decis√µes em contextos do mundo real. Desafios √©ticos podem surgir de _vieses do dataset_ (biases), problemas com a _qualidade de dados_, _injusti√ßa_, e _m√° representa√ß√£o_ nos algoritmos - incluindo alguns problemas que s√£o sist√™micos na natureza.

Em ambos os casos, desafios de √©tica destacam √°reas onde nossas a√ß√µes podem conflitar com nossos valores compartilhados. Para detectar, mitigar, minimizar, ou eliminar, essas preocupa√ß√µes - n√≥s precisamos perguntar quest√µes morais de "sim ou n√£o" relacionadas as nossas a√ß√µes, e ent√£o tomar uma a√ß√£o corretiva conforme necess√°rio. Vamos olhar alguns desafios √©ticos e as quest√µes morais que eles levantam:


#### 2.1 Propriedade de Dados

A coleta de dados geralmente envolve dados pessoais que podem identificar os titulares dos dados. [Propriedade de dados](https://permission.io/blog/data-ownership) √© sobre o _controle_ e [_direitos dos usu√°rios_](https://permission.io/blog/data-ownership) relacionados √† cria√ß√£o, processamento, e dissemina√ß√£o dos dados.

As quest√µes morais que precisamos nos perguntar s√£o:
 * Quem det√™m/possui os dados? (usu√°rio ou organiza√ß√£o)
 * Quais direitos os titulares dos dados tem? (ex: acesso, apagar, portabilidade)
 * Quais direitos as organiza√ß√µes tem? (ex: retificar reviews maliciosas de usu√°rios)

#### 2.2 Consentimento Informado

[Consentimento Informado](https://legaldictionary.net/informed-consent/) define o ato dos usu√°rios aceitar uma a√ß√£o (como a coleta de dados) com um _compreendimento total_ de fatos relevantes incluindo prop√≥sito, potenciais riscos, e alternativas.

Quest√µes a se explorar aqui s√£o:
 * O usu√°rio (titular dos dados) deu permiss√£o para a capta√ß√£o e uso dos dados?
 * O usu√°rio entendeu o prop√≥sito para o qual aqueles dados foram coletados?
 * O usu√°rio entendeu os potenciais riscos de sua participa√ß√£o?

#### 2.3 Propriedade Intelectual

[Propriedade intelectual](https://en.wikipedia.org/wiki/Intellectual_property) se refere a cria√ß√µes intang√≠veis que foram resultados das iniciativas humanas, que podem _ter valor econ√¥mico_ para indiv√≠duos ou neg√≥cios.

Quest√µes a se explorar aqui s√£o:
 * Os dados coletados tem valor econ√¥micos para um usu√°rio ou neg√≥cio?
 * O **usu√°rio** tem propriedade intelectual aqui?
 * As **organiza√ß√µes** tem propriedade intelectual aqui?
 * Se esses direitos existem, como estamos protejendo eles?

#### 2.4 Privacidade de Dados

[Privacidade de dados](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) ou privacidade da informa√ß√£o se refere a preserva√ß√£o da privacidade do usu√°rio e prote√ß√£o da identidade do usu√°rio com rela√ß√£o as informa√ß√µes de indentifica√ß√£o pessoal.

Quest√µes a se explorar aqui s√£o:
 * Os dados (pessoais) dos usu√°rios est√£o protegidos contra hacks e vazamentos?
 * Os dados do usu√°rio s√£o acess√≠veis somente a usu√°rios e contextos autorizados?
 * A anonimidade do usu√°rio s√£o preservados quando os dados s√£o compartilhados ou disseminados?
 * Um usu√°rio podem ser desindentificado de datasets an√¥nimos?


#### 2.5 Direito a Ser Esquecido

o [Direito a Ser Esquecido](https://en.wikipedia.org/wiki/Right_to_be_forgotten) ou [Direito de Apagar](https://www.gdpreu.org/right-to-be-forgotten/) fornecem prote√ß√µes de dados adicionais para os usu√°rios. Especificamente, d√° aos usu√°rios o direito de pedir dele√ß√£o ou remo√ß√£o dos dados pessoais das buscas da Internet e outros locais, _sobre circunst√¢ncias espec√≠ficas_ - permitindo a eles um novo come√ßo online sem que as a√ß√µes passadas sejam colocadas contra eles.

Quest√µes a se explorar aqui s√£o:
 * O sistema permite que os titulares dos dados pe√ßam o apagamento dos mesmos?
 * A retirada do consentimento do usu√°rio deve acionar um apagamento autom√°tico?
 * Dados foram colocados sem o consentimento ou por meios ilegais?
 * Estamos de acordo com regula√ß√µes governamentais para a privacidade de dados?


#### 2.6 Vi√©ses dos Datasets

[Vi√©ses da Cole√ß√£o ou do Dataset](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) √© sobre selecionar um subset de dados _n√£o representativos_ para o desenvolvimento de um algoritmo, criando potenciais injusti√ßas nos resultados para grupos diversos. Os tipos de vi√©ses incluem sele√ß√£o ou vi√©s da amostra, vi√©s volunt√°rio, e vi√©s do instrumento.

Quest√µes a se explorar aqui s√£o:
 * Recrutamos um conjunto representativo de titulares de dados?
 * N√≥s testamos nossos datasets colecionados ou com curadoria para diversos vi√©ses?
 * N√≥s podemos mitigar ou remover quaisquer vi√©ses descobertos?

#### 2.7 Qualidade de Dados

[Qualidade de Dados](https://lakefs.io/data-quality-testing/) procura pela validade do dataset com curadoria usado para desenvolver nossos algoritmos, checando para ver se recursos e registros atendem os requisitos para o n√≠vel de acur√°cia e consist√™ncia necess√°rios para o prop√≥sito da nossa IA.

Quest√µes a se explorar aqui s√£o:
 * N√≥s coletamos _features_ v√°lidos para nosso caso de uso?
 * Os dados foram coletados _consistentemente_ em diversas fontes de dados?
 * O dataset √© _completo_ para diversas condi√ß√µes e cen√°rios?
 * As informa√ß√µes capturadas refletem _com precis√£o_ a realidade?

#### 2.8 Justi√ßa do Algoritmo

[Justi√ßa do Algoritmo](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) checa para ver se o design do algoritmo discrimina sistematicamente subgrupos espec√≠ficos dos titulares dos dados levando a [potenciais danos](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) em _aloca√ß√£o_ (onde recursos s√£o negados ou detidos daquele grupo) e _qualidade de servi√ßo_ (onde IA n√£o √© t√£o acurada para alguns subgrupos quanto √© para outros).

Quest√µes a se explorar aqui s√£o:
 * N√≥s avaliamos a acur√°cia do modelo para diversos subgrupos e condi√ß√µes?
 * N√≥s examinamos o sistema em busca de danos potenciais (ex. estere√≥tipos)?
 * N√≥s podemos revisar os dados ou retreinar os modelos para mitigar danos identificados?

Explore recursos como [Checklist de Justi√ßa de IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para saber mais.

#### 2.9 M√° Representa√ß√£o

[M√° Representa√ß√£o dos Dados](https://www.sciencedirect.com/topics/computer-science/misrepresentation) √© sobre perguntar se n√≥s estamos comunicando insights de dados honestamente relatados de uma maneira enganosa para suportar uma narrativa desejada.

Quest√µes a se explorar aqui s√£o:
 * Estamos relatando dados completos ou inacurados?
 * Estamos visualizando dados de uma maneira que conduz a uma conclus√£o errada?
 * Estamos usando t√©cnicas estat√≠sticas seletivas para manipular os resultados?
 * Existem explica√ß√µes alternativas que podem oferecer uma conclus√£o diferente?

#### 2.10 Livre Escolha
A [Ilus√£o da Livre Escolha](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ocorre quando as "arquiteturas de escolha" do sistema utiliza algoritmos de tomada de decis√£o para incentivar as pessoas a obterem um resultado preferido enquanto parece lhe dar op√ß√µes e controle. Esses [dark patterns](https://www.darkpatterns.org/) podem causar danos sociais e econ√¥micos aos usu√°rios. J√° que as decis√µes do usu√°rio impactam perfis de comportamento, essas a√ß√µes potencialmente conduzem as escolhas futuras que podem aplificar ou extender o impacto desses danos.

Quest√µes a se explorar aqui s√£o:
 * O usu√°rio entende as implica√ß√µes de fazer aquela escolha?
 * O usu√°rio estava ciente das escolhas (alternativas) e dos pr√≥s e contras de cada uma?
 * O usu√°rio pode reverter um escolha automatizada ou influenciada depois?

### 3. Estudo de Casos

Para colocar esses desafios √©ticos em contextos do mundo real, ajuda olhar para estudo de casos que destacam potenciais danos e consequ√™ncias para indiv√≠duos e sociedade, quando essas viola√ß√µes √©ticas s√£o negligenciadas.

Aqui est√£o alguns exemplos:

| Desafios de √âticas | Estudo de Caso  | 
|--- |--- |
| **Consentimento Informado** | 1972 - [Tuskegee Syphillis Study](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Homens afro-americanos que participaram no estudo foram prometidos cuidados m√©dicos livres de custo _mas foram enganados_ pelos pesquisadores que n√£o informaram os participantes de seus diagn√≥sticos ou sobre a avaliabilidade de tratamentos. Muitos participantes morreram e parceiros e ciran√ßas foram afetados; oe studo durou por 40 anos. | 
| **Privacidade de Dados** |  2007 - O [Netflix data prize](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) forneceu a pesquisadores _10M de avalia√ß√µes an√¥nimas de filmes de 50K clientes_ para ajudar a melhorar os algoritmos de recomenda√ß√£o. No entanto, os pesquisadores conseguiram correlacionar os dados an√¥nimos com dados de identifica√ß√£o pessoal em _datasets externos_ (ex. coment√°rios no IMDb) - "desanonimizando" efetivamente alguns assinates da Netflix.|
| **Vi√©ses dos Datasets**  | 2013 - A Cidade de Boston [desenvolveu Street Bump](https://www.boston.gov/transportation/street-bump), um aplicativo que deixa os usu√°rios relatarem burcos nas ruas, dando √† cidade melhores dados rodovi√°rios para encontrar e consertar problemas. No entanto, [pessoas que faziam parte de grupos de baixa renda tinham menos acesso a carros e celulares](https://hbr.org/2013/04/the-hidden-biases-in-big-data), fazendo com que os seus problema rodovi√°rios fossem invis√≠veis nesse aplicativo. Desenvolvedores trabalharm com acad√™micos para quest√µes de _acesso equitativo e divis√µes digitais_ para justi√ßa. |
| **Justi√ßa do Algoritmo**  | 2018 - [O Gender Shades Study do MIT](http://gendershades.org/overview.html) avaliou a acur√°cia de produtos de IA de classifica√ß√£o de g√™neros, expondo lacunas na acur√°cia para mulheres e pessoas n√£o brancas. Um [Apple Card de 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) parece oferecer menos cr√©ditos para mulheres do que oferece para homens. Ambos ilustraram quest√µes de vi√©s algor√≠tmico levando a danos socioecon√¥micos.|
| **M√° Representa√ß√£o de Dados** | 2020 - O [Departamento de S√°ude P√∫blica da Georgia (Georgia Department of Public Health) liberou gr√°ficos da COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) que aparentam a levar os cidad√£os a conclus√µes err√¥neas sobre as tend√™ncias em casos confirmados em uma ordem n√£o cronol√≥gica no eixo x. Isso ilustra a m√° representa√ß√£o atr√°ves de truques de visualiza√ß√£o. |
| **Ilus√£o da Livre Escolha** | 2020 - Aplicativo de aprendizado [ABCmouse pagou $10M para resolver uma reclama√ß√£o do FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) onde os pais foram enganados a pagar assinaturas que eles n√£o podiam cancelar. Isso ilustra "dark patterns" em arquiteturas de escolha, onde usu√°rios foram direcionados a escolhas potencialmente prejudiciais. |
| **Privacidade de Dados & Direitos do Usu√°rio** | 2021 - [Viola√ß√£o de Dados do facebook](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) exp√¥s dados de mais de 530M de usu√°rios, resultando em um acordo de $5B com o FTC (Federal Trade Commission). No entanto, o Facebook se recusou a notificar os usu√°rios sobre a viola√ß√£o dos dados violando os direitos dos usu√°rios de transpar√™ncia e acesso de dados. |

Gostaria de explorar mais estudos de caso? Confira:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - dilemas √©ticos em ind√∫strias diversas.
* [Data Science Ethics course](https://www.coursera.org/learn/data-science-ethics#syllabus) - estudos de caso marcantes explorados.
* [Where things have gone wrong](https://deon.drivendata.org/examples/) - checklists da deon com exemplos

> üö® Pense sobre estudos de caso que voc√™ ja viu - voc√™ ja experienciou, ou foi afetado por, um desafio √©tico similar em sua vida? Voce consegue pensar em pelo menos um estudo de caso que ilustre um ou mais desafios √©ticos que discutimos nessa se√ß√£o?

## √âtica aplicada

N√≥s falamos sobre conceitos de √©ticas, desafios, e casos de estudo em contextos do mundo real. Mas como n√≥s come√ßamos a _aplicar_ esses princ√≠pios √©ticos em nossos projetos? E como n√≥s _operacionalizamos_ essas pr√°ticas para melhor govern√¢ncia? Vamos explorar algumas solu√ß√µes do mundo real:

### 1. C√≥digos Profissionais

C√≥digos Profisionais oferecem uma op√ß√£o para organiza√ß√µes para "incentivar" membros a apoiar os princ√≠pios √©ticos e frase de miss√£o. C√≥digos s√£o _diretrizes morais_ para comportamento profissional, ajudando funcion√°rios ou membros a tomar decis√µes que alinhem com os princ√≠pios da sua organiza√ß√£o. Eles s√£o t√£o bons quanto a conformidade volunt√°ria dos membros; no entanto, muitas organiza√ß√µes oferecem recompensas e penalidades adicionais para motivar a conformidade dos membros.

Exemplos incluem:

 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) C√≥digo de √âtica
 * [Data Science Association](http://datascienceassn.org/code-of-conduct.html) C√≥digo de Conduta (criado em 2013)
 * [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (desde 1993)

> üö® Voc√™ faz parte de uma organiza√ß√£o profissional de engenharia ou de ci√™ncias de dados? Explore o site deles para ver se eles definem um c√≥digo de √©tica profissional. O que diz sobre os princ√≠pios √©ticos deles? Como eles est√£o "incentivando" os membros a seguir o c√≥digo?

### 2. Checklists de √âticas

Enquanto c√≥digos profissionais definem _comportamentos √©tico_ requiridos de seus praticantes, eles [tem limita√ß√µes conhecidas](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) na execu√ß√£o, particularmente em projetos de larga escala. Ao inv√©s disso, muitos experts em Ci√™ncia de Dados [defendem as checklists](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), que podem **conectar princ√≠pios a pr√°ticas** de maneiras para determin√≠sticas e acion√°veis.

Checklists convertem as quest√µes em tarefas de "sim/n√£o" que podem ser operacionalizadas, permitindo eles serem rastreados como parte dos fluxos de trabalho de libera√ß√£o de produtos padr√£o. 

Exemplos incluem:
 * [Deon](https://deon.drivendata.org/) - uma checklist de prop√≥sito gerak criado a partir de [recomenda√ß√µes da ins√∫stria](https://deon.drivendata.org/#checklist-citations) com uma ferramenta de linha de comando para f√°cil integra√ß√£o.
 * [Privacy Audit Checklist](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - fornece orienta√ß√£o geral para pr√°ticas de manipula√ß√£o de informa√ß√£o a partir de perspectivas de exposi√ß√£o legal e social.
 * [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - criado por praticantes de IA para apoiar a ado√ß√£o e integra√ß√£o de verifica√ß√µes de justi√ßa dentro dos ciclos de desenvolvimento de IA.
 * [22 questions for ethics in data and AI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - estrutura mais aberto-fechado, estrturado para explora√ß√£o inicial de problemas √©ticos em contextos de design, implementa√ß√£o, e organizacional.

### 3. Regula√ß√µes √âticas

√âtica √© sobre definir valores compartilhados e fazer a coisa certa _voluntariamente_. **Compliance (Conformidade)** √© sobre _seguir a lei_ se e onde definida. **Govern√¢ncia** abrange amplamente todos as formas de como as organiza√ß√µes operam para garantir princ√≠pios √©ticos e cumprir as leis estabelecidas.

Hoje, govern√¢ncia assume duas formas dentro das organiza√ß√µes. Primeira, √© sobre definir princ√≠pios de **IA √©tica** e estabelecer pr√°ticas para operacionalizar a ado√ß√£o em todos os projetos de IA na organiza√ß√£o. Segundo, trata-se de cumprir com todos os **regulamentos de prote√ß√£o de dados** para as regi√µes em que operam.

Exemplos de prote√ß√£o de dados e regulamentos de privacidade:

 * `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - regula a coleta, o uso, e divulga√ß√£o de informa√ß√µes pessoais por parte do _governo federal_.
 * `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protege dados de s√°ude pessoais.
 * `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protege a privacidade de dados de crian√ßas menores de 13 anos de idade.
 * `2018`, [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) - fornece direitos aos usu√°rio, prote√ß√£o de dados, e privacidade.
 * `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) d√° aos consumidores mais _direitos_ sobre seus dados (pessoais).
 * `2021`, [A Lei de Prote√ß√£o de Informa√ß√£o Pessoal](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) da China acabou de ser passado, criando uma das regula√ß√µes de privacidade de dados online mais forte do mundo.

> üö® A GDPR (General Data Protection Regulation) da Uni√£o Europia continua sendo umas das regula√ß√µes de privacidade de dados mais influentes hoje em dia. Voc√™ sabia que a mesma tamb√©m define [8 direitos dos usu√°rio](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) para proteger a privacidade dos cidad√£os e dados pessoais? Saiba mais sobre o que s√£o e porque eles importam.


### 4. Cultura √âtica

Note que existe uma lacuna intang√≠vel entre _compliance_ (fazer o suficiente para cumprir a "a carta da lei") e abordar [problemas sist√™micos](https://www.coursera.org/learn/data-science-ethics/home/week/4) (como ossifica√ß√£o, assimetria informacional, e injusti√ßa distribucional) que podem acelerar o uso da IA como uma arma.

Este √∫ltimo requere [abordagens colaborativas para definir culturas √©ticas](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) que constr√≥i conex√µes emocionais e valores compartilhados consistentes _em todas as organiza√ß√µes_ na ind√∫stria. Isso requere mais [culturas de √©tica de dados formalizadas](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) nas organiza√ß√µes - permitindo _qualquer um_ a [puxar o cord√£o Andom](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (para aumentar as preocupa√ß√µes √©ticas mais cedo no processo) e fazendo _avalia√ß√µes √©ticas_ (ex. na contrata√ß√£o) um crit√©rio fundamental na forma√ß√£o de times em projetos de IA.

---
## [Quiz p√≥s aula](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/3) üéØ
## Revis√£o e Autoestudo

Cursos e livros ajudam a entender os conceitos essencias da √©tica, enquanto estudos de caso e ferramentas ajudam com pr√°ticas da √©tica aplicado em contextos do mundo real. Aqui est√£o alguns recursos para come√ßar.

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - aula sobre Justi√ßa, da Microsoft.
* [Principles of Responsible AI](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - programa de aprendizado gratuito da Microsoft Learn.
* [Ethics and Data Science](https://resources.oreilly.com/examples/0636920203964) - O'Reilly EBook (M. Loukides, H. Mason et. al)
* [Data Science Ethics](https://www.coursera.org/learn/data-science-ethics#syllabus) - curso online da Universidade de Michigan.
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - estudos de caso da Universidade do Texas.

# Tarefa 

[Escreva um Caso de Uso de √âtica de Dados](assignment.pt-br.md)
